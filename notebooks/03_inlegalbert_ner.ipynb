{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß† InLegalBERT Fine-Tuning for Legal NER\n",
                "\n",
                "This notebook fine-tunes InLegalBERT for Named Entity Recognition on Indian legal documents.\n",
                "\n",
                "**Entity Types:** PETITIONER, RESPONDENT, JUDGE, COURT, STATUTE, PROVISION, PRECEDENT, DATE, etc.\n",
                "\n",
                "**Model:** [law-ai/InLegalBERT](https://huggingface.co/law-ai/InLegalBERT)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "import json\n",
                "import torch\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForTokenClassification,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    DataCollatorForTokenClassification\n",
                ")\n",
                "from datasets import Dataset\n",
                "from seqeval.metrics import classification_report, f1_score\n",
                "\n",
                "from src.models import LegalNERModel\n",
                "from src.utils import set_seed\n",
                "\n",
                "# Set seed for reproducibility\n",
                "set_seed(42)\n",
                "\n",
                "# Check device\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Define Entity Labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Legal entity labels (BIO format)\n",
                "LABELS = [\n",
                "    \"O\",\n",
                "    \"B-PETITIONER\", \"I-PETITIONER\",\n",
                "    \"B-RESPONDENT\", \"I-RESPONDENT\",\n",
                "    \"B-JUDGE\", \"I-JUDGE\",\n",
                "    \"B-LAWYER\", \"I-LAWYER\",\n",
                "    \"B-COURT\", \"I-COURT\",\n",
                "    \"B-STATUTE\", \"I-STATUTE\",\n",
                "    \"B-PROVISION\", \"I-PROVISION\",\n",
                "    \"B-PRECEDENT\", \"I-PRECEDENT\",\n",
                "    \"B-CASE_NUMBER\", \"I-CASE_NUMBER\",\n",
                "    \"B-DATE\", \"I-DATE\",\n",
                "    \"B-GPE\", \"I-GPE\",\n",
                "    \"B-ORG\", \"I-ORG\",\n",
                "]\n",
                "\n",
                "label2id = {label: i for i, label in enumerate(LABELS)}\n",
                "id2label = {i: label for i, label in enumerate(LABELS)}\n",
                "\n",
                "print(f\"Total labels: {len(LABELS)}\")\n",
                "print(f\"Entity types: {len([l for l in LABELS if l.startswith('B-')])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model and Tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_NAME = \"law-ai/InLegalBERT\"\n",
                "\n",
                "# Load tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "\n",
                "# Load model for token classification\n",
                "model = AutoModelForTokenClassification.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    num_labels=len(LABELS),\n",
                "    id2label=id2label,\n",
                "    label2id=label2id\n",
                ")\n",
                "\n",
                "print(f\"Model loaded: {MODEL_NAME}\")\n",
                "print(f\"Parameters: {model.num_parameters():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Prepare Training Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample training data (replace with your annotated data)\n",
                "# Format: List of {\"tokens\": [...], \"ner_tags\": [...]}\n",
                "\n",
                "SAMPLE_DATA = [\n",
                "    {\n",
                "        \"tokens\": [\"In\", \"Kesavananda\", \"Bharati\", \"v.\", \"State\", \"of\", \"Kerala\", \",\", \n",
                "                   \"the\", \"Supreme\", \"Court\", \"examined\", \"Article\", \"368\", \".\"],\n",
                "        \"ner_tags\": [0, 1, 2, 0, 3, 4, 4, 0, 0, 9, 10, 0, 13, 14, 0]\n",
                "        # O, B-PETITIONER, I-PETITIONER, O, B-RESPONDENT, I-RESPONDENT, I-RESPONDENT, O, O, B-COURT, I-COURT, O, B-PROVISION, I-PROVISION, O\n",
                "    },\n",
                "    {\n",
                "        \"tokens\": [\"Hon'ble\", \"Justice\", \"D.Y.\", \"Chandrachud\", \"delivered\", \"the\", \"judgment\", \".\"],\n",
                "        \"ner_tags\": [5, 6, 6, 6, 0, 0, 0, 0]\n",
                "        # B-JUDGE, I-JUDGE, I-JUDGE, I-JUDGE, O, O, O, O\n",
                "    },\n",
                "    {\n",
                "        \"tokens\": [\"FIR\", \"No.\", \"123/2020\", \"was\", \"registered\", \"under\", \"Section\", \"302\", \"of\", \"the\", \"IPC\", \".\"],\n",
                "        \"ner_tags\": [17, 18, 18, 0, 0, 0, 13, 14, 0, 0, 11, 0]\n",
                "        # B-CASE_NUMBER, I-CASE_NUMBER, I-CASE_NUMBER, O, O, O, B-PROVISION, I-PROVISION, O, O, B-STATUTE, O\n",
                "    },\n",
                "]\n",
                "\n",
                "# Create dataset\n",
                "dataset = Dataset.from_list(SAMPLE_DATA)\n",
                "print(f\"Training samples: {len(dataset)}\")\n",
                "print(f\"\\nFirst sample:\")\n",
                "print(f\"  Tokens: {dataset[0]['tokens']}\")\n",
                "print(f\"  Tags: {[id2label[t] for t in dataset[0]['ner_tags']]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Tokenization and Alignment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tokenize_and_align_labels(examples):\n",
                "    \"\"\"\n",
                "    Tokenize and align labels with subword tokens.\n",
                "    \n",
                "    Key insight: When a word is split into subwords, only the first\n",
                "    subword gets the label, others get -100 (ignored in loss).\n",
                "    \"\"\"\n",
                "    tokenized = tokenizer(\n",
                "        examples[\"tokens\"],\n",
                "        truncation=True,\n",
                "        is_split_into_words=True,\n",
                "        max_length=512,\n",
                "        padding=\"max_length\"\n",
                "    )\n",
                "    \n",
                "    labels = []\n",
                "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
                "        word_ids = tokenized.word_ids(batch_index=i)\n",
                "        previous_word_idx = None\n",
                "        label_ids = []\n",
                "        \n",
                "        for word_idx in word_ids:\n",
                "            if word_idx is None:\n",
                "                # Special tokens get -100\n",
                "                label_ids.append(-100)\n",
                "            elif word_idx != previous_word_idx:\n",
                "                # First subword of a word\n",
                "                label_ids.append(label[word_idx])\n",
                "            else:\n",
                "                # Subsequent subwords get -100 or I- tag\n",
                "                label_ids.append(-100)\n",
                "            \n",
                "            previous_word_idx = word_idx\n",
                "        \n",
                "        labels.append(label_ids)\n",
                "    \n",
                "    tokenized[\"labels\"] = labels\n",
                "    return tokenized\n",
                "\n",
                "# Tokenize dataset\n",
                "tokenized_dataset = dataset.map(\n",
                "    tokenize_and_align_labels,\n",
                "    batched=True,\n",
                "    remove_columns=dataset.column_names\n",
                ")\n",
                "\n",
                "print(\"Dataset tokenized!\")\n",
                "print(f\"Features: {tokenized_dataset.features}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Define Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(eval_pred):\n",
                "    \"\"\"Compute NER metrics using seqeval.\"\"\"\n",
                "    predictions, labels = eval_pred\n",
                "    predictions = np.argmax(predictions, axis=2)\n",
                "    \n",
                "    # Remove ignored indices and convert to label strings\n",
                "    true_labels = []\n",
                "    true_predictions = []\n",
                "    \n",
                "    for pred, label in zip(predictions, labels):\n",
                "        true_label = []\n",
                "        true_pred = []\n",
                "        \n",
                "        for p, l in zip(pred, label):\n",
                "            if l != -100:\n",
                "                true_label.append(id2label[l])\n",
                "                true_pred.append(id2label[p])\n",
                "        \n",
                "        true_labels.append(true_label)\n",
                "        true_predictions.append(true_pred)\n",
                "    \n",
                "    return {\n",
                "        \"f1\": f1_score(true_labels, true_predictions),\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "OUTPUT_DIR = \"../models/legal_ner\"\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    num_train_epochs=10,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=16,\n",
                "    learning_rate=2e-5,\n",
                "    weight_decay=0.01,\n",
                "    warmup_steps=100,\n",
                "    logging_steps=10,\n",
                "    save_steps=500,\n",
                "    save_total_limit=2,\n",
                "    evaluation_strategy=\"no\",  # Change to \"steps\" if you have validation data\n",
                "    fp16=torch.cuda.is_available(),\n",
                "    report_to=\"tensorboard\",\n",
                ")\n",
                "\n",
                "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset,\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics,\n",
                ")\n",
                "\n",
                "print(\"Trainer configured!\")\n",
                "print(f\"Output directory: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Train Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚ö†Ô∏è Uncomment to train (requires more data for good results)\n",
                "# trainer.train()\n",
                "\n",
                "print(\"Training code ready!\")\n",
                "print(\"\\nTo train with real data:\")\n",
                "print(\"1. Replace SAMPLE_DATA with annotated NER data\")\n",
                "print(\"2. Uncomment trainer.train()\")\n",
                "print(\"3. Run the cell\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚ö†Ô∏è Uncomment after training\n",
                "# trainer.save_model(f\"{OUTPUT_DIR}/final\")\n",
                "# tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final\")\n",
                "\n",
                "print(\"Model save code ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Inference Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_entities(text: str, model, tokenizer, id2label):\n",
                "    \"\"\"Predict entities in text.\"\"\"\n",
                "    # Tokenize\n",
                "    inputs = tokenizer(\n",
                "        text,\n",
                "        return_tensors=\"pt\",\n",
                "        truncation=True,\n",
                "        max_length=512,\n",
                "        return_offsets_mapping=True\n",
                "    )\n",
                "    \n",
                "    offset_mapping = inputs.pop(\"offset_mapping\")[0].tolist()\n",
                "    \n",
                "    # Move to device\n",
                "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
                "    \n",
                "    # Predict\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "        predictions = torch.argmax(outputs.logits, dim=-1)[0].tolist()\n",
                "    \n",
                "    # Extract entities\n",
                "    entities = []\n",
                "    current_entity = None\n",
                "    \n",
                "    for idx, (pred, (start, end)) in enumerate(zip(predictions, offset_mapping)):\n",
                "        if start == end:  # Special token\n",
                "            continue\n",
                "        \n",
                "        label = id2label[pred]\n",
                "        \n",
                "        if label.startswith(\"B-\"):\n",
                "            if current_entity:\n",
                "                entities.append(current_entity)\n",
                "            current_entity = {\n",
                "                \"text\": text[start:end],\n",
                "                \"label\": label[2:],\n",
                "                \"start\": start,\n",
                "                \"end\": end\n",
                "            }\n",
                "        elif label.startswith(\"I-\") and current_entity:\n",
                "            current_entity[\"text\"] = text[current_entity[\"start\"]:end]\n",
                "            current_entity[\"end\"] = end\n",
                "        else:\n",
                "            if current_entity:\n",
                "                entities.append(current_entity)\n",
                "                current_entity = None\n",
                "    \n",
                "    if current_entity:\n",
                "        entities.append(current_entity)\n",
                "    \n",
                "    return entities\n",
                "\n",
                "# Test with sample text\n",
                "test_text = \"In Kesavananda Bharati v. State of Kerala, the Supreme Court examined Article 368.\"\n",
                "print(f\"Test text: {test_text}\")\n",
                "print(\"\\n(Run prediction after training)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "1. **Get More Data**: Use the Indian Kanoon scraper and annotate with Label Studio\n",
                "2. **Train**: Run the training cell with proper data\n",
                "3. **Evaluate**: Add validation set and check F1 scores\n",
                "4. **Deploy**: Use the model in the NER pipeline"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}