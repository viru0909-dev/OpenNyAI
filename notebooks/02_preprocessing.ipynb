{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ”§ Data Preprocessing for Indian Legal Documents\n",
                "\n",
                "This notebook covers:\n",
                "- Text cleaning and normalization\n",
                "- Citation standardization\n",
                "- Entity extraction with regex\n",
                "- PII anonymization\n",
                "- Preparing data for NER/RRL training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "import json\n",
                "import re\n",
                "from pathlib import Path\n",
                "from typing import Dict, List\n",
                "\n",
                "import pandas as pd\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "from src.data import LegalDataLoader, LegalTextPreprocessor, SemanticChunker\n",
                "from src.utils import IndianLegalPatterns, clean_legal_text\n",
                "\n",
                "# Paths\n",
                "RAW_DATA_DIR = Path('../data/raw')\n",
                "PROCESSED_DATA_DIR = Path('../data/processed')\n",
                "PROCESSED_DATA_DIR.mkdir(exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialize Preprocessing Components"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize components\n",
                "preprocessor = LegalTextPreprocessor(preserve_legal_terms=True)\n",
                "patterns = IndianLegalPatterns()\n",
                "chunker = SemanticChunker(chunk_size=512, chunk_overlap=50)\n",
                "\n",
                "print(\"âœ“ Preprocessor initialized\")\n",
                "print(\"âœ“ Regex patterns loaded\")\n",
                "print(\"âœ“ Semantic chunker ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Text Cleaning Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_legal_document(text: str, anonymize: bool = True) -> Dict:\n",
                "    \"\"\"\n",
                "    Full preprocessing pipeline for a legal document.\n",
                "    \n",
                "    Returns:\n",
                "        Dict with cleaned text, extracted entities, metadata\n",
                "    \"\"\"\n",
                "    # Step 1: Basic cleaning\n",
                "    cleaned = preprocessor.clean_text(text)\n",
                "    \n",
                "    # Step 2: Legal normalization (Section -> standardized)\n",
                "    normalized = preprocessor.normalize_legal_text(cleaned)\n",
                "    \n",
                "    # Step 3: Extract legal entities\n",
                "    entities = patterns.extract_legal_terms(normalized)\n",
                "    \n",
                "    # Step 4: Extract sections with Acts\n",
                "    sections_with_acts = patterns.extract_sections(normalized)\n",
                "    \n",
                "    # Step 5: PII anonymization (if enabled)\n",
                "    if anonymize:\n",
                "        final_text = patterns.anonymize_pii(normalized)\n",
                "    else:\n",
                "        final_text = normalized\n",
                "    \n",
                "    # Step 6: Sentence segmentation\n",
                "    sentences = preprocessor.segment_sentences(final_text)\n",
                "    \n",
                "    return {\n",
                "        'text': final_text,\n",
                "        'sentences': sentences,\n",
                "        'entities': entities,\n",
                "        'sections_with_acts': sections_with_acts,\n",
                "        'word_count': len(final_text.split()),\n",
                "        'sentence_count': len(sentences)\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the pipeline\n",
                "sample_text = \"\"\"\n",
                "In the case of  Ram Kumar    vs.   State of UP,  FIR No. 123/2020 was filed.\n",
                "The accused was charged u/s 302 of IPC and sec. 34 of Indian Penal Code.\n",
                "Contact: 9876543210. \n",
                "Citing AIR 2019 SC 456, the Hon'ble Court held...\n",
                "\"\"\"\n",
                "\n",
                "result = preprocess_legal_document(sample_text)\n",
                "\n",
                "print(\"CLEANED TEXT:\")\n",
                "print(result['text'])\n",
                "print(f\"\\nWord count: {result['word_count']}\")\n",
                "print(f\"Sentence count: {result['sentence_count']}\")\n",
                "print(f\"\\nSections with Acts: {result['sections_with_acts']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Semantic Chunking for Long Documents"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Long document example\n",
                "long_document = \"\"\"\n",
                "JUDGMENT\n",
                "\n",
                "1. This appeal challenges the judgment dated 15.01.2023 passed by the \n",
                "High Court of Delhi in WP(C) No. 1234/2022.\n",
                "\n",
                "2. The brief facts of the case are as follows:\n",
                "\n",
                "FACTS\n",
                "\n",
                "3. The petitioner is a company registered under the Companies Act, 2013. \n",
                "The petitioner entered into a contract with the respondent for supply of \n",
                "goods worth Rs. 50,00,000/-.\n",
                "\n",
                "4. The respondent failed to make payment despite repeated reminders. \n",
                "A legal notice was sent on 01.03.2022 demanding payment within 15 days.\n",
                "\n",
                "5. Subsequently, the petitioner filed a suit for recovery before the \n",
                "District Court, which was dismissed on technical grounds.\n",
                "\n",
                "ISSUE\n",
                "\n",
                "6. The main issue for determination is whether the High Court was \n",
                "justified in dismissing the writ petition on grounds of alternate remedy.\n",
                "\n",
                "ANALYSIS\n",
                "\n",
                "7. We have heard the learned counsel for both parties and perused the \n",
                "material on record.\n",
                "\n",
                "8. In ABL International Ltd. v. Export Credit Guarantee Corporation, \n",
                "(2004) 3 SCC 553, this Court held that writ jurisdiction should not be \n",
                "exercised in contractual matters.\n",
                "\n",
                "9. However, in appropriate cases where there is violation of principles \n",
                "of natural justice, writ remedy is available.\n",
                "\n",
                "ORDER\n",
                "\n",
                "10. In view of the above, the appeal is allowed in part.\n",
                "\"\"\"\n",
                "\n",
                "# Chunk the document\n",
                "chunks = chunker.chunk_legal_document(long_document, preserve_sections=True)\n",
                "\n",
                "print(f\"Document split into {len(chunks)} chunks:\\n\")\n",
                "for chunk in chunks:\n",
                "    print(f\"--- Chunk {chunk.chunk_id} (~{len(chunk.text)//4} tokens) ---\")\n",
                "    print(chunk.text[:150] + \"...\" if len(chunk.text) > 150 else chunk.text)\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Prepare NER Training Data Format"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_ner_annotation(text: str, entities: List[Dict]) -> Dict:\n",
                "    \"\"\"\n",
                "    Create NER annotation in spaCy/HuggingFace format.\n",
                "    \n",
                "    Format:\n",
                "    {\n",
                "        \"text\": \"...\",\n",
                "        \"entities\": [[start, end, label], ...]\n",
                "    }\n",
                "    \"\"\"\n",
                "    annotations = []\n",
                "    \n",
                "    for entity in entities:\n",
                "        start = entity.get('start', 0)\n",
                "        end = entity.get('end', 0)\n",
                "        label = entity.get('label', 'UNKNOWN')\n",
                "        \n",
                "        if start >= 0 and end > start:\n",
                "            annotations.append([start, end, label])\n",
                "    \n",
                "    return {\n",
                "        \"text\": text,\n",
                "        \"entities\": annotations\n",
                "    }\n",
                "\n",
                "# Example: Manual annotation format\n",
                "example_ner_data = [\n",
                "    {\n",
                "        \"text\": \"In Kesavananda Bharati v. State of Kerala, the Supreme Court examined Article 368.\",\n",
                "        \"entities\": [\n",
                "            [3, 22, \"PETITIONER\"],      # Kesavananda Bharati\n",
                "            [27, 42, \"RESPONDENT\"],     # State of Kerala\n",
                "            [48, 61, \"COURT\"],          # Supreme Court\n",
                "            [71, 82, \"PROVISION\"]       # Article 368\n",
                "        ]\n",
                "    },\n",
                "    {\n",
                "        \"text\": \"FIR No. 123/2020 was registered under Section 302 of the Indian Penal Code.\",\n",
                "        \"entities\": [\n",
                "            [0, 16, \"CASE_NUMBER\"],     # FIR No. 123/2020\n",
                "            [38, 49, \"PROVISION\"],      # Section 302\n",
                "            [57, 75, \"STATUTE\"]         # Indian Penal Code\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "print(\"Sample NER Training Data:\")\n",
                "print(json.dumps(example_ner_data, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Prepare Rhetorical Role Labeling Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# RRL Training Data Format\n",
                "example_rrl_data = [\n",
                "    {\n",
                "        \"sentence\": \"This appeal arises from the judgment dated 15.01.2023 passed by the High Court of Delhi.\",\n",
                "        \"label\": \"PREAMBLE\"\n",
                "    },\n",
                "    {\n",
                "        \"sentence\": \"The petitioner is a company registered under the Companies Act, 2013.\",\n",
                "        \"label\": \"FACTS\"\n",
                "    },\n",
                "    {\n",
                "        \"sentence\": \"The main issue for determination is whether the High Court was justified in dismissing the petition.\",\n",
                "        \"label\": \"ISSUE\"\n",
                "    },\n",
                "    {\n",
                "        \"sentence\": \"Learned counsel for the petitioner submitted that the order is arbitrary.\",\n",
                "        \"label\": \"ARGUMENT_PETITIONER\"\n",
                "    },\n",
                "    {\n",
                "        \"sentence\": \"We have carefully considered the submissions and perused the record.\",\n",
                "        \"label\": \"ANALYSIS\"\n",
                "    },\n",
                "    {\n",
                "        \"sentence\": \"In ABL International v. ECGC (2004) 3 SCC 553, this Court held...\",\n",
                "        \"label\": \"PRECEDENT_RELIED\"\n",
                "    },\n",
                "    {\n",
                "        \"sentence\": \"The doctrine of proportionality requires that restrictions must be necessary.\",\n",
                "        \"label\": \"RATIO\"\n",
                "    },\n",
                "    {\n",
                "        \"sentence\": \"In view of the above, the appeal is allowed.\",\n",
                "        \"label\": \"RULING_PRESENT_COURT\"\n",
                "    }\n",
                "]\n",
                "\n",
                "print(\"Sample RRL Training Data:\")\n",
                "for item in example_rrl_data:\n",
                "    print(f\"[{item['label']:25}] {item['sentence'][:60]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Batch Processing Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_documents_batch(documents: List[Dict], output_path: str) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Process a batch of documents and save to file.\n",
                "    \n",
                "    Args:\n",
                "        documents: List of dicts with 'id' and 'text' keys\n",
                "        output_path: Path to save processed data\n",
                "    \n",
                "    Returns:\n",
                "        DataFrame with processing statistics\n",
                "    \"\"\"\n",
                "    processed = []\n",
                "    stats = []\n",
                "    \n",
                "    for doc in tqdm(documents, desc=\"Processing documents\"):\n",
                "        doc_id = doc.get('id', 'unknown')\n",
                "        text = doc.get('text', '')\n",
                "        \n",
                "        if not text:\n",
                "            continue\n",
                "        \n",
                "        # Process document\n",
                "        result = preprocess_legal_document(text)\n",
                "        \n",
                "        # Store processed\n",
                "        processed.append({\n",
                "            'id': doc_id,\n",
                "            'text': result['text'],\n",
                "            'sentences': result['sentences'],\n",
                "            'entities': result['entities']\n",
                "        })\n",
                "        \n",
                "        # Collect stats\n",
                "        stats.append({\n",
                "            'id': doc_id,\n",
                "            'word_count': result['word_count'],\n",
                "            'sentence_count': result['sentence_count'],\n",
                "            'num_citations': len(result['entities'].get('citations', [])),\n",
                "            'num_statutes': len(result['entities'].get('acts', []))\n",
                "        })\n",
                "    \n",
                "    # Save processed data\n",
                "    with open(output_path, 'w', encoding='utf-8') as f:\n",
                "        json.dump(processed, f, ensure_ascii=False, indent=2)\n",
                "    \n",
                "    print(f\"\\nâœ“ Saved {len(processed)} documents to {output_path}\")\n",
                "    \n",
                "    return pd.DataFrame(stats)\n",
                "\n",
                "# Example usage (with dummy data)\n",
                "sample_docs = [\n",
                "    {\"id\": \"doc_001\", \"text\": \"In the Supreme Court, the petitioner filed under Article 32.\"},\n",
                "    {\"id\": \"doc_002\", \"text\": \"FIR No. 456/2021 was registered u/s 420 IPC at PS Sadar.\"},\n",
                "]\n",
                "\n",
                "# stats_df = process_documents_batch(sample_docs, 'data/processed/sample_processed.json')\n",
                "# stats_df.describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Export Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_for_ner_training(data: List[Dict], output_path: str, format: str = 'jsonl'):\n",
                "    \"\"\"Save data in format suitable for NER training.\"\"\"\n",
                "    if format == 'jsonl':\n",
                "        with open(output_path, 'w', encoding='utf-8') as f:\n",
                "            for item in data:\n",
                "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "    else:\n",
                "        with open(output_path, 'w', encoding='utf-8') as f:\n",
                "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
                "    \n",
                "    print(f\"âœ“ Saved {len(data)} samples to {output_path}\")\n",
                "\n",
                "# Save examples\n",
                "# save_for_ner_training(example_ner_data, '../data/processed/ner_samples.jsonl', format='jsonl')\n",
                "# save_for_ner_training(example_rrl_data, '../data/processed/rrl_samples.jsonl', format='jsonl')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "1. **Collect Data**: Run the Indian Kanoon scraper\n",
                "2. **Annotate**: Use Label Studio or similar for NER annotations\n",
                "3. **Train**: Use the InLegalBERT fine-tuning notebooks"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}