{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ü¶ô Llama 3 Fine-Tuning with LoRA for Legal Reasoning\n",
                "\n",
                "This notebook fine-tunes Llama 3 using LoRA/QLoRA for Indian legal tasks:\n",
                "- Legal question answering\n",
                "- Case summarization\n",
                "- Argument drafting\n",
                "- Legal language simplification\n",
                "\n",
                "**Based on:** Aalap project methodology"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "import torch\n",
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "from src.training import LlamaLoRATrainer, LoRAConfig\n",
                "from src.utils import set_seed, setup_logging\n",
                "\n",
                "# Configuration\n",
                "set_seed(42)\n",
                "setup_logging(log_level=\"INFO\")\n",
                "\n",
                "# Check GPU\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
                "    print(\"Using Apple MPS\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configure LoRA Parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LoRA Configuration\n",
                "config = LoRAConfig(\n",
                "    # LoRA hyperparameters\n",
                "    r=16,                    # Rank (8, 16, 32, 64)\n",
                "    lora_alpha=32,           # Scaling factor\n",
                "    lora_dropout=0.05,       # Dropout\n",
                "    \n",
                "    # Target all linear modules for better reasoning\n",
                "    target_modules=[\n",
                "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
                "    ],\n",
                "    \n",
                "    # Quantization (QLoRA)\n",
                "    use_4bit=True,           # Enable 4-bit for memory efficiency\n",
                "    \n",
                "    # Training\n",
                "    learning_rate=2e-4,\n",
                "    batch_size=4,\n",
                "    gradient_accumulation_steps=4,  # Effective batch size = 16\n",
                "    num_epochs=3,\n",
                "    max_seq_length=2048,\n",
                ")\n",
                "\n",
                "print(\"LoRA Configuration:\")\n",
                "print(f\"  Rank (r): {config.r}\")\n",
                "print(f\"  Alpha: {config.lora_alpha}\")\n",
                "print(f\"  4-bit quantization: {config.use_4bit}\")\n",
                "print(f\"  Target modules: {len(config.target_modules)} layers\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prepare Instruction Dataset (Aalap Format)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample instruction data (Aalap format)\n",
                "SAMPLE_INSTRUCTIONS = [\n",
                "    {\n",
                "        \"instruction\": \"Summarize the key legal issues in this case.\",\n",
                "        \"input\": \"\"\"The petitioner has filed a writ petition under Article 226 challenging \n",
                "        the order of termination from service. The petitioner was employed as a Grade B \n",
                "        officer and was terminated without a show cause notice. The petitioner contends \n",
                "        that the termination violates the principles of natural justice.\"\"\",\n",
                "        \"output\": \"\"\"This case involves a challenge to service termination under Article 226. \n",
                "        The key issues are: (1) Whether the termination without show cause notice violates \n",
                "        principles of natural justice, and (2) Whether the petitioner has a right to be heard \n",
                "        before adverse action.\"\"\"\n",
                "    },\n",
                "    {\n",
                "        \"instruction\": \"Draft an argument for the petitioner based on Section 302 IPC.\",\n",
                "        \"input\": \"\"\"The accused was charged under Section 302 IPC for murder. The prosecution \n",
                "        claims the accused shot the deceased during a property dispute. The defense argues \n",
                "        that the accused acted in self-defense when the deceased attacked him with a knife.\"\"\",\n",
                "        \"output\": \"\"\"Argument for the Petitioner:\n",
                "\n",
                "1. The prosecution has failed to establish mens rea (guilty intention) beyond reasonable doubt.\n",
                "\n",
                "2. The accused's actions fall within the ambit of private defense under Section 96-106 IPC, as:\n",
                "   - The deceased was the aggressor\n",
                "   - The threat to life was imminent\n",
                "   - The force used was proportionate\n",
                "\n",
                "3. In Darshan Singh v. State of Punjab (2010) 2 SCC 333, this Hon'ble Court held that the \n",
                "right of private defense extends to causing death when there is reasonable apprehension.\n",
                "\n",
                "4. The charge should be reduced to Section 304 Part I (culpable homicide not amounting to murder).\"\"\"\n",
                "    },\n",
                "    {\n",
                "        \"instruction\": \"Simplify this legal text for a common person.\",\n",
                "        \"input\": \"\"\"The res judicata principle operates as a bar to subsequent litigation \n",
                "        where the matter in issue has been directly and substantially in issue in a former \n",
                "        suit between the same parties and has been heard and finally decided.\"\"\",\n",
                "        \"output\": \"\"\"In simple terms: Once a court has made a final decision on a dispute \n",
                "        between two parties, those same parties cannot file another case about the same issue \n",
                "        in any court. This is to prevent endless litigation and ensure finality.\"\"\"\n",
                "    },\n",
                "    {\n",
                "        \"instruction\": \"What are the legal grounds for bail under Section 437 CrPC?\",\n",
                "        \"input\": \"\",\n",
                "        \"output\": \"\"\"Under Section 437 CrPC, bail may be granted in non-bailable offences when:\n",
                "\n",
                "1. There are reasonable grounds to believe the accused is not guilty\n",
                "2. The accused is a woman, child under 16, or sick/infirm person\n",
                "3. The investigation is complete and no custodial interrogation is needed\n",
                "4. Maximum punishment is less than 7 years imprisonment\n",
                "\n",
                "However, bail cannot be granted if:\n",
                "- The accused is charged with an offence punishable with death or life imprisonment\n",
                "- The accused has previous convictions for similar offences\n",
                "- There is risk of tampering with evidence or influencing witnesses\"\"\"\n",
                "    }\n",
                "]\n",
                "\n",
                "# Save sample data\n",
                "sample_data_path = Path(\"../data/corpora/sample_instructions.jsonl\")\n",
                "sample_data_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "with open(sample_data_path, 'w', encoding='utf-8') as f:\n",
                "    for item in SAMPLE_INSTRUCTIONS:\n",
                "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "\n",
                "print(f\"Saved {len(SAMPLE_INSTRUCTIONS)} instruction samples to {sample_data_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initialize Trainer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize trainer\n",
                "# Note: For Llama 3, you need access from Meta. Use a smaller model for testing.\n",
                "\n",
                "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"  # Requires access\n",
                "# Alternative for testing: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "\n",
                "trainer = LlamaLoRATrainer(\n",
                "    model_name=MODEL_NAME,\n",
                "    config=config,\n",
                "    output_dir=\"../models/legal_llama\"\n",
                ")\n",
                "\n",
                "print(f\"Trainer initialized for: {MODEL_NAME}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Model (QLoRA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚ö†Ô∏è Uncomment to load (requires ~8GB GPU memory for 8B model with 4-bit)\n",
                "# trainer.load_model()\n",
                "\n",
                "print(\"Model loading code ready!\")\n",
                "print(\"\\nMemory requirements (approximate):\")\n",
                "print(\"  - Llama 3 8B (4-bit): ~6-8 GB VRAM\")\n",
                "print(\"  - Llama 3 70B (4-bit): ~40 GB VRAM\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Prepare Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare dataset from JSONL file\n",
                "# dataset = trainer.prepare_dataset(str(sample_data_path))\n",
                "# print(f\"Dataset prepared with {len(dataset)} samples\")\n",
                "\n",
                "print(\"Dataset preparation code ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train with LoRA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚ö†Ô∏è Uncomment to train\n",
                "# metrics = trainer.train(dataset)\n",
                "# print(f\"Training complete! Final metrics: {metrics}\")\n",
                "\n",
                "print(\"Training code ready!\")\n",
                "print(\"\\nExpected training time:\")\n",
                "print(\"  - 1000 samples, 3 epochs, A100: ~30 minutes\")\n",
                "print(\"  - 1000 samples, 3 epochs, RTX 3090: ~2 hours\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save LoRA Adapter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚ö†Ô∏è Uncomment after training\n",
                "# trainer.save_model()\n",
                "\n",
                "# Optional: Merge LoRA with base model for deployment\n",
                "# trainer.merge_and_save(\"../models/legal_llama_merged\")\n",
                "\n",
                "print(\"Save code ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Inference Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example inference (after training)\n",
                "def test_legal_reasoning():\n",
                "    instruction = \"What are the essential ingredients of Section 420 IPC (cheating)?\"\n",
                "    \n",
                "    response = trainer.generate(\n",
                "        instruction=instruction,\n",
                "        input_text=\"\",\n",
                "        max_new_tokens=256,\n",
                "        temperature=0.7\n",
                "    )\n",
                "    \n",
                "    print(f\"Instruction: {instruction}\")\n",
                "    print(f\"\\nResponse:\\n{response}\")\n",
                "\n",
                "# ‚ö†Ô∏è Uncomment after training\n",
                "# test_legal_reasoning()\n",
                "\n",
                "print(\"Inference code ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Using Groq for Fast Inference\n",
                "\n",
                "For production deployment, use Groq's blazing-fast LPU inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Set your Groq API key\n",
                "# os.environ[\"GROQ_API_KEY\"] = \"your-api-key-here\"\n",
                "\n",
                "def query_groq_llama(prompt: str):\n",
                "    \"\"\"Query Llama 3 via Groq API.\"\"\"\n",
                "    try:\n",
                "        from groq import Groq\n",
                "        \n",
                "        client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
                "        \n",
                "        response = client.chat.completions.create(\n",
                "            model=\"llama3-70b-8192\",\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": \"You are an AI legal assistant specialized in Indian law.\"},\n",
                "                {\"role\": \"user\", \"content\": prompt}\n",
                "            ],\n",
                "            temperature=0.3,\n",
                "            max_tokens=1024\n",
                "        )\n",
                "        \n",
                "        return response.choices[0].message.content\n",
                "    except Exception as e:\n",
                "        return f\"Error: {e}\"\n",
                "\n",
                "# Test Groq inference\n",
                "# result = query_groq_llama(\"Explain the concept of 'bail' in Indian criminal law.\")\n",
                "# print(result)\n",
                "\n",
                "print(\"Groq inference code ready!\")\n",
                "print(\"Set GROQ_API_KEY environment variable to use.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "1. **Get Aalap Dataset**: Download from [HuggingFace](https://huggingface.co/datasets/opennyaiorg/aalap_instruction_dataset)\n",
                "2. **Fine-tune**: Run training with full dataset\n",
                "3. **Evaluate**: Test on legal reasoning benchmarks\n",
                "4. **Deploy**: Use vLLM or Groq for serving"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}